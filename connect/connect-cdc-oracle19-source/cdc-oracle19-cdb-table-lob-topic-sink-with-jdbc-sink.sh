#!/bin/bash
set -e

DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null && pwd )"
source ${DIR}/../../scripts/utils.sh

create_or_get_oracle_image "LINUX.X64_193000_db_home.zip" "../../connect/connect-cdc-oracle19-source/ora-setup-scripts-cdb-table"

${DIR}/../../environment/plaintext/start.sh "${PWD}/docker-compose.plaintext.cdb-table.lob-topic-sink-with-jdbc-sink.yml"


# Verify Oracle DB has started within MAX_WAIT seconds
MAX_WAIT=2500
CUR_WAIT=0
log "âŒ› Waiting up to $MAX_WAIT seconds for Oracle DB to start"
docker container logs oracle > /tmp/out.txt 2>&1
while [[ ! $(cat /tmp/out.txt) =~ "DATABASE IS READY TO USE" ]]; do
sleep 10
docker container logs oracle > /tmp/out.txt 2>&1
CUR_WAIT=$(( CUR_WAIT+10 ))
if [[ "$CUR_WAIT" -gt "$MAX_WAIT" ]]; then
     logerror "ERROR: The logs in oracle container do not show 'DATABASE IS READY TO USE' after $MAX_WAIT seconds. Please troubleshoot with 'docker container ps' and 'docker container logs'.\n"
     exit 1
fi
done
log "Oracle DB has started!"

log "Setting up Oracle Database Prerequisites"
docker exec -i oracle bash -c "ORACLE_SID=ORCLCDB;export ORACLE_SID;sqlplus /nolog" << EOF
     CONNECT sys/Admin123 AS SYSDBA
     CREATE ROLE C##CDC_PRIVS;
     GRANT CREATE SESSION TO C##CDC_PRIVS;
     GRANT EXECUTE ON SYS.DBMS_LOGMNR TO C##CDC_PRIVS;
     GRANT LOGMINING TO C##CDC_PRIVS;
     GRANT SELECT ON V_\$LOGMNR_CONTENTS TO C##CDC_PRIVS;
     GRANT SELECT ON V_\$DATABASE TO C##CDC_PRIVS;
     GRANT SELECT ON V_\$THREAD TO C##CDC_PRIVS;
     GRANT SELECT ON V_\$PARAMETER TO C##CDC_PRIVS;
     GRANT SELECT ON V_\$NLS_PARAMETERS TO C##CDC_PRIVS;
     GRANT SELECT ON V_\$TIMEZONE_NAMES TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_INDEXES TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_OBJECTS TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_USERS TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_CATALOG TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_CONSTRAINTS TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_CONS_COLUMNS TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_TAB_COLS TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_IND_COLUMNS TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_ENCRYPTED_COLUMNS TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_LOG_GROUPS TO C##CDC_PRIVS;
   --  GRANT SELECT ON ALL_TAB_PARTITIONS TO C##CDC_PRIVS;
   --  GRANT SELECT ON SYS.DBA_REGISTRY TO C##CDC_PRIVS;
     GRANT SELECT ON SYS.OBJ\$ TO C##CDC_PRIVS;
   --  GRANT SELECT ON DBA_TABLESPACES TO C##CDC_PRIVS;
   --  GRANT SELECT ON DBA_OBJECTS TO C##CDC_PRIVS;
   --  GRANT SELECT ON SYS.ENC\$ TO C##CDC_PRIVS;
     GRANT SELECT ANY TABLE TO C##CDC_PRIVS;

     -- Following privileges are required additionally for 19c compared to 12c.
     GRANT SELECT ON V_\$ARCHIVED_LOG TO C##CDC_PRIVS;
     GRANT SELECT ON V_\$LOG TO C##CDC_PRIVS;
     GRANT SELECT ON V_\$LOGFILE TO C##CDC_PRIVS;
     GRANT SELECT ON V_\$INSTANCE to C##CDC_PRIVS;

     CREATE USER C##MYUSER IDENTIFIED BY mypassword DEFAULT TABLESPACE USERS;
     ALTER USER C##MYUSER QUOTA UNLIMITED ON USERS;

     GRANT C##CDC_PRIVS to C##MYUSER;

     GRANT CREATE TABLE TO C##MYUSER container=all;
     GRANT CREATE SEQUENCE TO C##MYUSER container=all;
     GRANT CREATE TRIGGER TO C##MYUSER container=all;
     GRANT FLASHBACK ANY TABLE TO C##MYUSER container=all;
     GRANT EXECUTE ON SYS.DBMS_LOGMNR TO C##CDC_PRIVS;
     GRANT EXECUTE ON SYS.DBMS_LOGMNR_D TO C##CDC_PRIVS;
     GRANT EXECUTE ON SYS.DBMS_LOGMNR_LOGREP_DICT TO C##CDC_PRIVS;
     ;
     
     -- Enable Supplemental Logging for All Columns
     ALTER SESSION SET CONTAINER=cdb\$root;
     ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;
     ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
     exit;
EOF

log "Inserting initial data"
docker exec -i oracle sqlplus C\#\#MYUSER/mypassword@//localhost:1521/ORCLCDB << EOF

     create table XML_INTERFACE (
          id NUMBER(10) GENERATED BY DEFAULT ON NULL AS IDENTITY (START WITH 42) NOT NULL,
          first_name VARCHAR(50),
          last_name VARCHAR(50),
          email VARCHAR(50),
          gender VARCHAR(50),
          club_status VARCHAR(20),
          comments CLOB,
          create_ts timestamp DEFAULT CURRENT_TIMESTAMP ,
          update_ts timestamp
     );

     ALTER TABLE XML_INTERFACE ADD (
     CONSTRAINT XML_INTERFACE_PK PRIMARY KEY (ID,first_name));

     CREATE OR REPLACE TRIGGER TRG_XML_INTERFACE_UPD
     BEFORE INSERT OR UPDATE ON XML_INTERFACE
     REFERENCING NEW AS NEW_ROW
     FOR EACH ROW
     BEGIN
     SELECT SYSDATE
          INTO :NEW_ROW.UPDATE_TS
          FROM DUAL;
     END;
     /

     insert into XML_INTERFACE (id, first_name, last_name, email, gender, club_status, comments) values (1, 'Rica', 'Blaisdell', 'rblaisdell0@rambler.ru', 'Female', 'bronze', utl_raw.cast_to_raw('Universal optimal hierarchy'));
     exit;
EOF

log "Creating Oracle source connector"
curl -X PUT \
     -H "Content-Type: application/json" \
     --data '{
               "connector.class": "io.confluent.connect.oracle.cdc.OracleCdcSourceConnector",
               "tasks.max":2,
               "key.converter": "io.confluent.connect.avro.AvroConverter",
               "key.converter.schema.registry.url": "http://schema-registry:8081",
               "value.converter": "io.confluent.connect.avro.AvroConverter",
               "value.converter.schema.registry.url": "http://schema-registry:8081",
               "confluent.license": "",
               "confluent.topic.bootstrap.servers": "broker:9092",
               "confluent.topic.replication.factor": "1",
               "oracle.server": "oracle",
               "oracle.port": 1521,
               "oracle.sid": "ORCLCDB",
               "oracle.username": "C##MYUSER",
               "oracle.password": "mypassword",
               "start.from":"snapshot",
               "redo.log.topic.name": "redo-log-topic",
               "redo.log.consumer.bootstrap.servers":"broker:9092",
               "table.inclusion.regex": ".*XML_INTERFACE.*",
               "table.topic.name.template": "public.${tableName}",
               "numeric.mapping": "best_fit",
               "connection.pool.max.size": 20,
               "redo.log.row.fetch.size":1,
               "oracle.dictionary.mode": "auto",
               "topic.creation.redo.include": "redo-log-topic",
               "topic.creation.redo.replication.factor": 1,
               "topic.creation.redo.partitions": 1,
               "topic.creation.redo.cleanup.policy": "delete",
               "topic.creation.redo.retention.ms": 1209600000,
               "topic.creation.default.replication.factor": 1,
               "topic.creation.default.partitions": 1,
               "topic.creation.default.cleanup.policy": "delete",
               "lob.topic.name.template": "public.XML_INTERFACE_CLOB"
          }' \
     http://localhost:8083/connectors/cdc-oracle-source-cdb/config | jq .

log "Waiting 20s for connector to read existing data"
sleep 5


docker exec -i oracle sqlplus C\#\#MYUSER/mypassword@//localhost:1521/ORCLCDB << EOF
     insert into XML_INTERFACE (id, first_name, last_name, email, gender, club_status, comments) values (3, 'Rica2', 'Blai2sdell', 'rblaisde2ll0@rambler.ru', 'Fema2le', 'bronze', utl_raw.cast_to_raw('Universal2 optimal hierarchy'));
     exit;
EOF



set +e
playground topic consume --topic XML_INTERFACE --min-expected-messages 1 --timeout 60

playground topic consume --topic redo-log-topic --min-expected-messages 1 --timeout 60

playground topic consume --topic XML_INTERFACE_CLOB --min-expected-messages 1 --timeout 60

curl --request PUT \
  --url http://localhost:8083/admin/loggers/io.confluent.connect.jdbc \
  --header 'Accept: application/json' \
  --header 'Content-Type: application/json' \
  --data '{
 "level": "TRACE"
}'

log "Creating JDBC PostgreSQL sink connector"
curl -X PUT \
     -H "Content-Type: application/json" \
     --data '{
               "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
               "tasks.max": "1",
               "connection.url": "jdbc:postgresql://postgres/postgres?user=myuser&password=mypassword&ssl=false",
               "topics": "public.XML_INTERFACE_CLOB,public.XML_INTERFACE",
               "auto.create": "true",
               "key.converter": "io.confluent.connect.avro.AvroConverter",
               "key.converter.schema.registry.url": "http://schema-registry:8081",
               "value.converter": "io.confluent.connect.avro.AvroConverter",
               "value.converter.schema.registry.url": "http://schema-registry:8081",
               "delete.enabled": "true",
               "insert.mode": "UPSERT",
               "pk.mode": "record_key",
               "table.name.format": "${topic}",
               "table.types": "TABLE",

               "predicates": "isXML_INTERFACE,isXML_INTERFACE_CLOB",
               "predicates.isXML_INTERFACE.pattern": "public.XML_INTERFACE",
               "predicates.isXML_INTERFACE.type": "org.apache.kafka.connect.transforms.predicates.TopicNameMatches",
               "predicates.isXML_INTERFACE_CLOB.pattern": "public.XML_INTERFACE_CLOB",
               "predicates.isXML_INTERFACE_CLOB.type": "org.apache.kafka.connect.transforms.predicates.TopicNameMatches",

               "transforms": "HoistField,flattenKey,flattenValue",

               "transforms.HoistField.type": "org.apache.kafka.connect.transforms.HoistField$Value",
               "transforms.HoistField.field": "XML_MESSAGE",
               "transforms.HoistField.predicate": "isXML_INTERFACE_CLOB",

               "transforms.flattenKey.type": "org.apache.kafka.connect.transforms.Flatten$Key",
               "transforms.flattenKey.delimiter": ".",

               "transforms.flattenValue.type": "org.apache.kafka.connect.transforms.Flatten$Value",
               "transforms.flattenValue.delimiter": ".",
               "transforms.flattenValue.predicate": "isXML_INTERFACE"
          }' \
     http://localhost:8083/connectors/postgres-sink/config | jq .


# [2023-01-23 12:08:36,486] ERROR [postgres-sink|task-0] WorkerSinkTask{id=postgres-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: null (STRUCT) type doesn't have a mapping to the SQL database column type (org.apache.kafka.connect.runtime.WorkerSinkTask:616)
# org.apache.kafka.connect.errors.ConnectException: null (STRUCT) type doesn't have a mapping to the SQL database column type
# 	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.getSqlType(GenericDatabaseDialect.java:1945)
# 	at io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect.getSqlType(PostgreSqlDatabaseDialect.java:332)
# 	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnSpec(GenericDatabaseDialect.java:1861)
# 	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.lambda$writeColumnsSpec$39(GenericDatabaseDialect.java:1850)
# 	at io.confluent.connect.jdbc.util.ExpressionBuilder.append(ExpressionBuilder.java:560)
# 	at io.confluent.connect.jdbc.util.ExpressionBuilder$BasicListBuilder.of(ExpressionBuilder.java:599)
# 	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.writeColumnsSpec(GenericDatabaseDialect.java:1852)
# 	at io.confluent.connect.jdbc.dialect.GenericDatabaseDialect.buildCreateTableStatement(GenericDatabaseDialect.java:1769)
# 	at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:121)
# 	at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:67)
# 	at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:122)
# 	at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
# 	at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
# 	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:584)
# 	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:334)
# 	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:235)
# 	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:204)
# 	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:201)
# 	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:256)
# 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
# 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
# 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
# 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
# 	at java.base/java.lang.Thread.run(Thread.java:829)


# [2023-01-10 11:00:54,604] ERROR [postgres-sink|task-0] WorkerSinkTask{id=postgres-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:208)
# org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
#         at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:618)
#         at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:334)
#         at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:235)
#         at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:204)
#         at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:201)
#         at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:256)
#         at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
#         at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
#         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
#         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
#         at java.base/java.lang.Thread.run(Thread.java:829)
# Caused by: org.apache.kafka.connect.errors.ConnectException: Value schema must be of type Struct
#         at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:86)
#         at io.confluent.connect.jdbc.sink.metadata.FieldsMetadata.extract(FieldsMetadata.java:67)
#         at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:115)
#         at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:74)
#         at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:84)
#         at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:584)
#         ... 10 more

sleep 4

log "Show content of XML_INTERFACE_CLOB table:"
docker exec postgres bash -c "psql -U myuser -d postgres -c 'SELECT * FROM \"XML_INTERFACE_CLOB\"'"

log "Show content of XML_INTERFACE table:"
docker exec postgres bash -c "psql -U myuser -d postgres -c 'SELECT * FROM \"XML_INTERFACE\"'"

#                                                       payload                                                       |  column  | primary_key |            table            
# --------------------------------------------------------------------------------------------------------------------+----------+-------------+-----------------------------
#  \x353536453639373636353732373336313643323036463730373436393644363136433230363836393635373236313732363336383739     | XML_INTERFACE_CLOB |           1 | ORCLCDB.C##MYUSER.XML_INTERFACE
#  \x3535364536393736363537323733363136433332323036463730373436393644363136433230363836393635373236313732363336383739 | XML_INTERFACE_CLOB |           2 | ORCLCDB.C##MYUSER.XML_INTERFACE
# (2 rows)

# [2023-01-10 13:58:07,478] INFO [postgres-sink|task-0] Creating table with sql: CREATE TABLE "XML_INTERFACE_CLOB" (
# "payload" BYTEA NOT NULL,
# "column" TEXT NOT NULL,
# "primary_key" BIGINT NOT NULL,
# "table" TEXT NOT NULL,
# PRIMARY KEY("column","primary_key","table")) (io.confluent.connect.jdbc.sink.DbStructure:122)
